{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6212a897-1a14-4981-b8ee-d41aa3f6a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 34603008 bytes (45061547223 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/nih-chest-xrays/data?dataset_version_number=3 (34603008/45096150231) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100M/42.0G [02:37<28:02:45, 446kB/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnih-chest-xrays/data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath to dataset files:\u001b[39m\u001b[38;5;124m\"\u001b[39m, path)\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\kagglehub\\datasets.py:28\u001b[0m, in \u001b[0;36mdataset_download\u001b[1;34m(handle, path, force_download)\u001b[0m\n\u001b[0;32m     26\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_dataset_handle(handle)\n\u001b[0;32m     27\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\kagglehub\\registry.py:23\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\kagglehub\\http_resolver.py:125\u001b[0m, in \u001b[0;36mDatasetHttpResolver.__call__\u001b[1;34m(self, h, path, force_download)\u001b[0m\n\u001b[0;32m    122\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(archive_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# First, we download the archive.\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Create the directory to extract the archive to.\u001b[39;00m\n\u001b[0;32m    128\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(out_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\kagglehub\\clients.py:188\u001b[0m, in \u001b[0;36mKaggleApiV1Client.download_file\u001b[1;34m(self, path, out_file, resource_handle, cached_path)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    181\u001b[0m         response\u001b[38;5;241m.\u001b[39murl,  \u001b[38;5;66;03m# URL after redirection\u001b[39;00m\n\u001b[0;32m    182\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_read\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    186\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m resumed_response:\n\u001b[0;32m    187\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResuming download from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize_read\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) bytes left.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 188\u001b[0m         \u001b[43m_download_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresumed_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\kagglehub\\clients.py:231\u001b[0m, in \u001b[0;36m_download_file\u001b[1;34m(response, out_file, size_read, total_size, hash_object)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mtotal_size, initial\u001b[38;5;241m=\u001b[39msize_read, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_file, open_mode) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 231\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhash_object\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\ASHUTOSH KUMAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"nih-chest-xrays/data\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48741e4-1418-49d6-91b6-78bf2b33049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: C:/Users/lenovo/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3\n",
      "Folders in dataset: ['ARXIV_V5_CHESTXRAY.pdf', 'BBox_List_2017.csv', 'Data_Entry_2017.csv', 'FAQ_CHESTXRAY.pdf', 'images_001', 'images_002', 'images_003', 'images_004', 'images_005', 'images_006', 'images_007', 'images_008', 'images_009', 'images_010', 'images_011', 'images_012', 'LOG_CHESTXRAY.pdf', 'README_CHESTXRAY.pdf', 'test_list.txt', 'train_val_list.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Path to the dataset (replace with your path)\n",
    "data_path = 'C:/Users/lenovo/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3'\n",
    "print(\"Dataset path:\", data_path)\n",
    "\n",
    "# Check the directory structure\n",
    "folders = os.listdir(data_path)\n",
    "print(\"Folders in dataset:\", folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922875e-ddfe-4972-8f70-5b7b2fefa35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files in the dataset: ['BBox_List_2017.csv', 'Data_Entry_2017.csv']\n",
      "        Image Index Finding Label     Bbox [x           y           w  \\\n",
      "0  00013118_008.png   Atelectasis  225.084746  547.019217   86.779661   \n",
      "1  00014716_007.png   Atelectasis  686.101695  131.543498  185.491525   \n",
      "2  00029817_009.png   Atelectasis  221.830508  317.053115  155.118644   \n",
      "3  00014687_001.png   Atelectasis  726.237288  494.951420  141.016949   \n",
      "4  00017877_001.png   Atelectasis  660.067797  569.780787  200.677966   \n",
      "\n",
      "           h]  Unnamed: 6  Unnamed: 7  Unnamed: 8  \n",
      "0   79.186441         NaN         NaN         NaN  \n",
      "1  313.491525         NaN         NaN         NaN  \n",
      "2  216.949153         NaN         NaN         NaN  \n",
      "3   55.322034         NaN         NaN         NaN  \n",
      "4   78.101695         NaN         NaN         NaN  \n",
      "Total records: 984\n"
     ]
    }
   ],
   "source": [
    "# Look for any CSV files that might contain labels or metadata\n",
    "csv_files = [f for f in os.listdir(data_path) if f.endswith('.csv')]\n",
    "print(\"CSV files in the dataset:\", csv_files)\n",
    "\n",
    "# If a CSV file is found, you can load it and inspect the data\n",
    "if csv_files:\n",
    "    csv_file = os.path.join(data_path, csv_files[0])\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(df.head())  # Display the first few rows\n",
    "    print(f\"Total records: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2cede-72fc-458b-a28a-54711a6e16ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Label\n",
      "Atelectasis     180\n",
      "Effusion        153\n",
      "Cardiomegaly    146\n",
      "Infiltrate      123\n",
      "Pneumonia       120\n",
      "Pneumothorax     98\n",
      "Mass             85\n",
      "Nodule           79\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Finding Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2094bf6-7809-4edd-aef3-cee9bfca99a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 112120 image files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the dataset\n",
    "data_path = 'C:/Users/lenovo/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3'\n",
    "image_folders = [f'images_{i:03d}' for i in range(1, 13)]  # Generating folder names like images_001, images_002, etc.\n",
    "\n",
    "# List to store image file paths\n",
    "image_paths = []\n",
    "\n",
    "# Collect image paths from all subfolders\n",
    "for folder in image_folders:\n",
    "    folder_path = os.path.join(data_path, folder, 'images')\n",
    "    \n",
    "    if os.path.isdir(folder_path):  # Check if the folder exists\n",
    "        images = os.listdir(folder_path)\n",
    "        image_paths.extend([os.path.join(folder_path, img) for img in images])\n",
    "\n",
    "print(f\"Found {len(image_paths)} image files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7e94d-1fd1-4689-8bfd-57f60eb2b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid images found: 100 out of 100\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Validate a subset of image paths\n",
    "valid_image_paths = []\n",
    "for img_path in image_paths[:100]:  # Check the first 100 paths (or all if needed)\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img.verify()  # Ensure the file is a valid image\n",
    "        valid_image_paths.append(img_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Invalid image file: {img_path}, Error: {e}\")\n",
    "\n",
    "print(f\"Valid images found: {len(valid_image_paths)} out of {len(image_paths[:100])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75917192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36f62e53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4484a5-d3bc-4342-ae1a-f337b36aceaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Example usage: Preprocess the first 10 images\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m preprocessed_images \u001b[38;5;241m=\u001b[39m [preprocess_image(img_path) \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mimage_paths\u001b[49m[:\u001b[38;5;241m10\u001b[39m]]\n\u001b[0;32m     16\u001b[0m preprocessed_images \u001b[38;5;241m=\u001b[39m [img \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m preprocessed_images \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]  \u001b[38;5;66;03m# Remove None values\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of preprocessed images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(preprocessed_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_paths' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "IMG_SIZE = 224  # Resize dimensions\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).resize((IMG_SIZE, IMG_SIZE))\n",
    "        img_array = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {image_path}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage: Preprocess the first 10 images\n",
    "preprocessed_images = [preprocess_image(img_path) for img_path in image_paths[:10]]\n",
    "preprocessed_images = [img for img in preprocessed_images if img is not None]  # Remove None values\n",
    "print(f\"Number of preprocessed images: {len(preprocessed_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376dfd6-e8e5-45db-8a38-6727da3b5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to the dataset directory\n",
    "data_path = 'C:/Users/lenovo/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3'\n",
    "\n",
    "# List all files in the dataset directory\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2f0c4-3ca2-4ac6-a6ff-d046d78e4100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
      "0  00000001_000.png            Cardiomegaly            0           1   \n",
      "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
      "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
      "3  00000002_000.png              No Finding            0           2   \n",
      "4  00000003_000.png                  Hernia            0           3   \n",
      "\n",
      "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
      "0           58              M            PA                 2682     2749   \n",
      "1           58              M            PA                 2894     2729   \n",
      "2           58              M            PA                 2500     2048   \n",
      "3           81              M            PA                 2500     2048   \n",
      "4           81              F            PA                 2582     2991   \n",
      "\n",
      "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
      "0                        0.143  0.143          NaN  \n",
      "1                        0.143  0.143          NaN  \n",
      "2                        0.168  0.168          NaN  \n",
      "3                        0.171  0.171          NaN  \n",
      "4                        0.143  0.143          NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'C:/Users/lenovo/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3/Data_Entry_2017.csv'\n",
    "\n",
    "# Load the CSV into a pandas dataframe\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows to understand its structure\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b8d5e-eb7f-443a-b0d3-e336d7e3bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataset\n",
    "print(\"Shape of the dataset:\", df.shape)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Get unique labels in the 'Finding Labels' column\n",
    "print(\"Unique labels:\\n\", df['Finding Labels'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199841f6-7b88-4f0e-a6f8-cfe28dcb6b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000001_000.png</td>\n",
       "      <td>Cardiomegaly</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2682</td>\n",
       "      <td>2749</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000001_001.png</td>\n",
       "      <td>Cardiomegaly|Emphysema</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2894</td>\n",
       "      <td>2729</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000001_002.png</td>\n",
       "      <td>Cardiomegaly|Effusion</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000002_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000003_000.png</td>\n",
       "      <td>Hernia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2582</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
       "0  00000001_000.png            Cardiomegaly            0           1   \n",
       "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
       "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
       "3  00000002_000.png              No Finding            0           2   \n",
       "4  00000003_000.png                  Hernia            0           3   \n",
       "\n",
       "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "0           58              M            PA                 2682     2749   \n",
       "1           58              M            PA                 2894     2729   \n",
       "2           58              M            PA                 2500     2048   \n",
       "3           81              M            PA                 2500     2048   \n",
       "4           81              F            PA                 2582     2991   \n",
       "\n",
       "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
       "0                        0.143  0.143          NaN  \n",
       "1                        0.143  0.143          NaN  \n",
       "2                        0.168  0.168          NaN  \n",
       "3                        0.171  0.171          NaN  \n",
       "4                        0.143  0.143          NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c726bea-4d63-4409-ab77-047c7789988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Image Index', 'Finding Labels', 'Follow-up #', 'Patient ID', 'Patient Age', 'Patient Gender', 'View Position', 'OriginalImage[Width', 'Height]', 'OriginalImagePixelSpacing[x', 'y]', 'Unnamed: 11']\n"
     ]
    }
   ],
   "source": [
    "# List of column names\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Print the column names\n",
    "print(column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa3f28-1f2e-4e2d-b781-dff1970e4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Unnamed: 11', 'Follow-up #', 'Patient ID', 'OriginalImage[Width','Height]', 'OriginalImagePixelSpacing[x', 'y]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdd351c-505d-4743-ac64-df705714ade3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiLabelBinarizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the MultiLabelBinarizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Initialize the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Convert the 'Finding Labels' column into a list of labels\n",
    "labels = df['Finding Labels'].str.split('|')\n",
    "\n",
    "# Fit and transform the labels\n",
    "binary_labels = mlb.fit_transform(labels)\n",
    "\n",
    "# Create a new DataFrame for the binary encoded labels\n",
    "binary_labels_df = pd.DataFrame(binary_labels, columns=mlb.classes_)\n",
    "\n",
    "# Concatenate the binary labels with the original dataframe (excluding the 'Finding Labels' column)\n",
    "df = pd.concat([df.drop('Finding Labels', axis=1), binary_labels_df], axis=1)\n",
    "\n",
    "# Show the first few rows to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee6cae-059b-4bf5-af57-01470bba5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "# Label Encoding for 'Patient Gender'\n",
    "le_gender = LabelEncoder()\n",
    "df['Patient Gender'] = le_gender.fit_transform(df['Patient Gender'])\n",
    "\n",
    "# One-Hot Encoding for 'View Position'\n",
    "df = pd.get_dummies(df, columns=['View Position'], drop_first=True)\n",
    "\n",
    "# Standard Scaling for 'Patient Age'\n",
    "scaler = StandardScaler()\n",
    "df['Patient Age'] = scaler.fit_transform(df[['Patient Age']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f948bfe-9716-48b1-95ba-a973737f2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Image Index', 'Patient Age', 'Patient Gender', 'Atelectasis',\n",
      "       'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema',\n",
      "       'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'No Finding', 'Nodule',\n",
      "       'Pleural_Thickening', 'Pneumonia', 'Pneumothorax', 'View Position_PA'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the final columns\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d0a67-09a2-469e-90b9-56dcd454f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224  # Resize dimensions\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path).resize((IMG_SIZE, IMG_SIZE))\n",
    "        img_array = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {img_path}, Error: {e}\")\n",
    "        return None\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88afff-dc84-43f7-b12d-209c9bdedeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (4999, 224, 224, 3)\n",
      "Labels shape: (4999, 15)\n",
      "Train Images: (3999, 224, 224, 3), Train Labels: (3999, 15)\n",
      "Test Images: (1000, 224, 224, 3), Test Labels: (1000, 15)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Path to the dataset directory and CSV file\n",
    "# Path to the image directory\n",
    "image_directory = 'C:/Users/lenovo/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3/images_001/images'\n",
    "\n",
    "# Filter the dataset to include only images available in the directory\n",
    "available_images = set(os.listdir(image_directory))\n",
    "df = df[df['Image Index'].isin(available_images)]\n",
    "\n",
    "# Drop unnecessary columns\n",
    "labels_columns = df.columns[4:]  # Assuming first 4 columns are non-label data\n",
    "df_labels = df[labels_columns].astype(np.float32)\n",
    "labels = df_labels.values\n",
    "\n",
    "# Prepare image paths and preprocess images\n",
    "image_paths = df['Image Index'].tolist()\n",
    "images = []\n",
    "\n",
    "for img_name in image_paths:\n",
    "    img_path = os.path.join(image_directory, img_name)\n",
    "    try:\n",
    "        # Load and preprocess the image\n",
    "        img = load_img(img_path, target_size=(224, 224))\n",
    "        img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
    "        images.append(img_array)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {img_path}\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Ensure consistency between images and labels\n",
    "if images.shape[0] != labels.shape[0]:\n",
    "    raise ValueError(\"Mismatch between number of images and labels.\")\n",
    "\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Split into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, train_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train Images: {train_images.shape}, Train Labels: {train_labels.shape}\")\n",
    "print(f\"Test Images: {test_images.shape}, Test Labels: {test_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa53ed-e4ab-4be6-92fa-bdc9ec8791c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(15, activation='sigmoid')  # 15 labels; sigmoid for multi-label classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbac4f1-0612-4794-8077-f7d19200ffb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Use a pre-trained model (EfficientNetB0)\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(15, activation='sigmoid')(x)  # Multi-label classification\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Freeze base model layers for transfer learning\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041564d-5df1-43a7-ae8a-e65d60eae802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow(train_images, train_labels, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657c768-ffba-46f9-83f2-0faf60e7fa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights for each label: {0: {np.float32(0.0): np.float64(0.5213734415198892), np.float32(1.0): np.float64(12.19675925925926)}, 1: {np.float32(0.0): np.float64(0.520240916271722), np.float32(1.0): np.float64(12.851219512195122)}, 2: {np.float32(0.0): np.float64(0.5086889360880479), np.float32(1.0): np.float64(29.272222222222222)}, 3: {np.float32(0.0): np.float64(0.5520746018440905), np.float32(1.0): np.float64(5.300804828973843)}, 4: {np.float32(0.0): np.float64(0.5121500777604977), np.float32(1.0): np.float64(21.076)}, 5: {np.float32(0.0): np.float64(0.5178887359937094), np.float32(1.0): np.float64(14.475274725274724)}, 6: {np.float32(0.0): np.float64(0.5298672566371682), np.float32(1.0): np.float64(8.87037037037037)}, 7: {np.float32(0.0): np.float64(0.600250626566416), np.float32(1.0): np.float64(2.99375)}, 8: {np.float32(0.0): np.float64(0.5177869496855346), np.float32(1.0): np.float64(14.55524861878453)}, 9: {np.float32(0.0): np.float64(1.0475149105367794), np.float32(1.0): np.float64(0.9566085693536674)}, 10: {np.float32(0.0): np.float64(0.5211671612265084), np.float32(1.0): np.float64(12.310747663551401)}, 11: {np.float32(0.0): np.float64(0.5161637931034483), np.float32(1.0): np.float64(15.966666666666667)}, 12: {np.float32(0.0): np.float64(0.5062451960030746), np.float32(1.0): np.float64(40.53076923076923)}, 13: {np.float32(0.0): np.float64(0.5196252465483234), np.float32(1.0): np.float64(13.238693467336683)}, 14: {np.float32(0.0): np.float64(1.5415447630193095), np.float32(1.0): np.float64(0.7400280898876405)}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights for each label\n",
    "class_weights_dict = {}\n",
    "\n",
    "for i in range(labels.shape[1]):  # Iterate over each class\n",
    "    y = labels[:, i]  # Binary labels for class i\n",
    "    unique_classes = np.unique(y)  # Ensure unique classes (0 and 1)\n",
    "    \n",
    "    if len(unique_classes) > 1:  # To avoid errors for classes with only one value\n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=unique_classes,\n",
    "            y=y\n",
    "        )\n",
    "        class_weights_dict[i] = {unique_classes[0]: class_weights[0], unique_classes[1]: class_weights[1]}\n",
    "    else:\n",
    "        print(f\"Skipping class {i}, only one class present.\")\n",
    "\n",
    "print(\"Class weights for each label:\", class_weights_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569229d3-a243-4715-964d-0906eb2a8f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset size: 5099\n"
     ]
    }
   ],
   "source": [
    "# Find indices of underrepresented samples\n",
    "minority_class_indices = np.where(labels[:, 6] == 1)[0]  # Example for class 6\n",
    "oversample_indices = np.random.choice(minority_class_indices, size=100, replace=True)  # Oversample\n",
    "\n",
    "# Append oversampled images and labels\n",
    "oversampled_images = np.concatenate([images, images[oversample_indices]], axis=0)\n",
    "oversampled_labels = np.concatenate([labels, labels[oversample_indices]], axis=0)\n",
    "\n",
    "print(f\"New dataset size: {oversampled_images.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09700d35-b874-411c-aaec-a85099f65a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset size: 5269\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    shear_range=0.15\n",
    ")\n",
    "\n",
    "# Generate augmented data for the minority class (class 6)\n",
    "minority_class_indices = np.where(labels[:, 6] == 1)[0]\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "for idx in minority_class_indices:\n",
    "    img = images[idx]\n",
    "    label = labels[idx]\n",
    "    for _ in range(10):  # Generate 10 augmented samples per image\n",
    "        augmented_img = datagen.random_transform(img)\n",
    "        augmented_images.append(augmented_img)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "# Append augmented data to the dataset\n",
    "images = np.concatenate([images, np.array(augmented_images)], axis=0)\n",
    "labels = np.concatenate([labels, np.array(augmented_labels)], axis=0)\n",
    "\n",
    "print(f\"Augmented dataset size: {images.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72089b-6514-4f83-a47c-c56736426dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "\n",
    "# Load ResNet50 model pre-trained on ImageNet\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# Add custom layers on top\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "output = Dense(labels.shape[1], activation='sigmoid')(x)  # Multi-label classification\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Freeze base model layers for transfer learning\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f510c-fc39-4bb7-9abb-947156342680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def weighted_binary_crossentropy(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        return -tf.reduce_mean(weights[0] * y_true * tf.math.log(y_pred) +\n",
    "                               weights[1] * (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# Example: Assign weights for imbalance (adjust for your data)\n",
    "positive_weight = 0.9\n",
    "negative_weight = 0.1\n",
    "\n",
    "model.compile(optimizer='adam', loss=weighted_binary_crossentropy([negative_weight, positive_weight]), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab91777-04e7-4e61-9e40-d153e24ce0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_252']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 2s/step - accuracy: 0.1253 - loss: 0.0823 - val_accuracy: 0.0160 - val_loss: 0.0471\n",
      "Epoch 2/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 2s/step - accuracy: 0.0639 - loss: 0.0461 - val_accuracy: 0.0160 - val_loss: 0.0464\n",
      "Epoch 3/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 2s/step - accuracy: 0.1197 - loss: 0.0446 - val_accuracy: 0.0160 - val_loss: 0.0465\n",
      "Epoch 4/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 3s/step - accuracy: 0.1711 - loss: 0.0447 - val_accuracy: 0.0430 - val_loss: 0.0454\n",
      "Epoch 5/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1290s\u001b[0m 10s/step - accuracy: 0.1101 - loss: 0.0440 - val_accuracy: 0.0680 - val_loss: 0.0459\n",
      "Epoch 6/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 3s/step - accuracy: 0.1449 - loss: 0.0445 - val_accuracy: 0.0160 - val_loss: 0.0458\n",
      "Epoch 7/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 3s/step - accuracy: 0.1352 - loss: 0.0435 - val_accuracy: 0.2920 - val_loss: 0.0453\n",
      "Epoch 8/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 3s/step - accuracy: 0.1335 - loss: 0.0444 - val_accuracy: 0.2730 - val_loss: 0.0448\n",
      "Epoch 9/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 3s/step - accuracy: 0.1532 - loss: 0.0429 - val_accuracy: 0.1090 - val_loss: 0.0443\n",
      "Epoch 10/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1214s\u001b[0m 10s/step - accuracy: 0.1282 - loss: 0.0434 - val_accuracy: 0.0320 - val_loss: 0.0434\n",
      "Epoch 11/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m408s\u001b[0m 3s/step - accuracy: 0.1632 - loss: 0.0429 - val_accuracy: 0.0370 - val_loss: 0.0433\n",
      "Epoch 12/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 4s/step - accuracy: 0.1753 - loss: 0.0427 - val_accuracy: 0.1430 - val_loss: 0.0427\n",
      "Epoch 13/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 4s/step - accuracy: 0.1613 - loss: 0.0440 - val_accuracy: 0.3090 - val_loss: 0.0449\n",
      "Epoch 14/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 4s/step - accuracy: 0.1563 - loss: 0.0423 - val_accuracy: 0.2690 - val_loss: 0.0439\n",
      "Epoch 15/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m557s\u001b[0m 4s/step - accuracy: 0.1829 - loss: 0.0417 - val_accuracy: 0.0530 - val_loss: 0.0425\n",
      "Epoch 16/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 3s/step - accuracy: 0.1399 - loss: 0.0419 - val_accuracy: 0.1300 - val_loss: 0.0422\n",
      "Epoch 17/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 3s/step - accuracy: 0.1530 - loss: 0.0411 - val_accuracy: 0.0770 - val_loss: 0.0418\n",
      "Epoch 18/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.1766 - loss: 0.0423 - val_accuracy: 0.0590 - val_loss: 0.0424\n",
      "Epoch 19/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 2s/step - accuracy: 0.1649 - loss: 0.0420 - val_accuracy: 0.0980 - val_loss: 0.0420\n",
      "Epoch 20/20\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 2s/step - accuracy: 0.1623 - loss: 0.0417 - val_accuracy: 0.1520 - val_loss: 0.0415\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    shear_range=0.15\n",
    ")\n",
    "\n",
    "# Apply augmentation to train images\n",
    "datagen.fit(train_images)\n",
    "history = model.fit(datagen.flow(train_images, train_labels, batch_size=32),\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e69db-8905-42dd-8e68-1472f4d3e0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.1470 - loss: 0.0406\n",
      "Test Loss: 0.04147966951131821\n",
      "Test Accuracy: 0.15199999511241913\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbb456",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "704f2f2f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788e0ebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the entire model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchest_xray_classifier.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Alternatively, save model weights\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchest_xray_model_weights.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "model.save('chest_xray_classifier.h5')\n",
    "\n",
    "# Alternatively, save model weights\n",
    "model.save_weights('chest_xray_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106490e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def predict_chest_xray(image_path):\n",
    "    # Load the saved model\n",
    "    model = load_model('chest_xray_classifier.h5')\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(img_array)[0]\n",
    "    \n",
    "    # Define label names (make sure this matches your training)\n",
    "    labels = [\n",
    "        'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "        'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', \n",
    "        'Infiltration', 'Mass', 'No Finding', 'Nodule', \n",
    "        'Pleural_Thickening', 'Pneumonia', 'Pneumothorax'\n",
    "    ]\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        label: float(prob) for label, prob in zip(labels, predictions)\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
